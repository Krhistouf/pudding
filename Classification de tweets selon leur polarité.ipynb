{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectifs\n",
    "\n",
    "- **Manipulation de textes :** transformations diverses pour en produire différentes représentations.\n",
    "\n",
    "- **Utilisation d'une librairie dédiée à l'apprentissage automatique** : entraîner et évaluer des modèles de classification automatique des textes préparés.\n",
    "\n",
    "- **Implémentation d'une méthode de classification** qui ne fait pas appel à l'apprentissage automatique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description de la tâche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification multi-classe de tweets en français en trois classes de polarité : positif, négatif, neutre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Installation et) importation des outils nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "# from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk : en général limité quant au traitement du français\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En ligne de commande (sans le signe d'exclamation) ou dans une cellule du notebook (avec le signe d'exclamation)\n",
    "# !pip install -U spacy\n",
    "## !python -m spacy download fr\n",
    "# !python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy : bonne couverture du français ; conçu spécifiquement pour s'interfacer avec des frameworks de deep learning\n",
    "import spacy\n",
    "nlp = spacy.load('fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (A) Récupération et mise en forme des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Données du défi [DEFT2015](https://deft.limsi.fr/2015/corpus.fr.php?lang=fr) : tweets rédigés en français, portant sur la thématique des changements climatiques. Tweets annotés selon leur polarité, pour la tâche 1 du défi : \"Classification des tweets selon leur polarité. Étant donné un tweet, cette tâche consiste à le classer, selon l’opinion/sentiment/émotion qu'il exprime, en positif, négatif, neutre ou mixte (si le tweet contient à la fois un sentiment positif et un sentiment négatif).\"\n",
    "\n",
    "__ATTENTION : Ces jeux de données ont été mis à notre disposition EXCLUSIVEMENT à des fins pédagogiques par les organisateurs du défi DEFT 2015. Leur redistribution est formellement interdite et tout travail écrit (rapport de stage, article, etc.) produit sur la base de ces données devra citer les sources indiquées sur le [site Web de DEFT 2015](https://deft.limsi.fr/2015/corpus.fr.php?lang=fr).__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Structure des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données en apprentissage automatique sont généralement séparées en trois jeux :\n",
    "\n",
    "+ **entraînement** : données destinées à l'apprentissage du modèle ;\n",
    "\n",
    "+ **validation** : données destinées à une évaluation intermédiaire du modèle pour permettre l'ajustement de ses hyperparamètres ;\n",
    "\n",
    "+ **test** : données destinées EXCLUSIVEMENT à l'évaluation FINALE (à réaliser une fois uniquement !) du modèle choisi finalement. Elles ne doivent sous aucune forme servir à la conception du modèle. Il est donc interdit aussi bien de les examiner que d'évaluer le modèle en cours de développement sur ce jeu de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans notre cas, deux jeux de données sont fournis :\n",
    "+ entraînement (appelé *Train*), consistant en 7929 observations ;\n",
    "+ test (appelé *Test*), contenant 3379 observations, soit environ 43% de la taille du jeu d'entraînement.\n",
    "\n",
    "Nous garderons cette répartition des données, même si un jeu de test plus petit pourrait probablement convenir (ce qui nous permettrait d'exploiter une partie du jeu de test pour l'entraînement). Mais nous devrons prendre soin de ménager nous-mêmes un jeu de validation. À vous d'évaluer votre modèle FINAL sur les données de test !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les étiquettes (la vérité terrain) et le texte des tweets sont stockés séparément. Nous les regrouperons à partir de l'identifiant du tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Étiquettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_label_file_path(parent_dir, train_or_test):\n",
    "    return os.path.join(parent_dir,\n",
    "                        f'{train_or_test.title()}_References',\n",
    "                        'T1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_label_file_path('data', 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_label_file_path('data', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_label_to_numeric(label):\n",
    "    return 1 if label == '+' else 0 if label == '=' else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(parent_dir, train_or_test):\n",
    "    label_file = make_label_file_path(parent_dir, train_or_test)\n",
    "    labels = pd.read_table(label_file, header=None, names=['id', 'polarity'])\n",
    "    labels['polarity'] = labels['polarity'].apply(map_label_to_numeric)\n",
    "    labels.set_index('id', inplace=True)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = get_labels('data', 'train')\n",
    "test_labels = get_labels('data', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_dir_path(parent_dir, train_or_test):\n",
    "    return os.path.join(parent_dir, f'deft2015_{train_or_test.upper()}_twitter_raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(parent_dir, train_or_test):\n",
    "    if train_or_test == 'test':\n",
    "        train_or_test += 's' # incohérence dans le nommage des répertoires\n",
    "    data_dir = make_data_dir_path(parent_dir, train_or_test)\n",
    "    tweets = dict()\n",
    "\n",
    "    for file_name in sorted(os.listdir(data_dir)):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            with open(os.path.join(data_dir, file_name), 'r') as f:\n",
    "                text = f.read().strip()\n",
    "            id_tweet = int(os.path.splitext(file_name)[0])\n",
    "            tweets[id_tweet] = text\n",
    "    \n",
    "    return (pd.DataFrame.from_dict(tweets, orient='index')\n",
    "                        .rename(columns={0: 'text'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = 'data/twitter'\n",
    "train_tweets = get_tweets(parent_dir, 'train')\n",
    "test_tweets = get_tweets(parent_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__*QUESTION : Combien de tweets y a-t-il dans le jeu de données d'entraînement et dans celui de test ?*__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_tweets), len(test_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Jonction des textes et des étiquettes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a plus d'étiquettes que de textes, car des tweets ont pu disparaître entre le moment où ils ont été collectés pour l'annotation de référence et le moment où ils ont été récupérés ultérieurement (voir [ici](https://deft.limsi.fr/2015/evaluation.fr.php?lang=fr)). Nous ferons une jointure interne pour ne retenir que les éléments communs aux deux tableaux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cependant, tous les tweets disponibles ont une étiquette :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(train_tweets.index).intersection(set(train_labels.index))) == len(train_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tweets_and_labels(tweets_df, labels_df):\n",
    "    return pd.merge(tweets_df, labels_df, how='inner',\n",
    "                    left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = merge_tweets_and_labels(train_tweets, train_labels)\n",
    "test_tweets = merge_tweets_and_labels(test_tweets, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est une bonne idée de sauvegarder régulièrement les data frames, pour pouvoir continuer à travailler dessus plus tard si besoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets.to_pickle('train.pkl')\n",
    "test_tweets.to_pickle('test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour lire les fichiers si besoin :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tweets = pd.read_pickle('train.pkl')\n",
    "# test_tweets = pd.read_pickle('test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploration des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Distribution des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distribution = (pd.DataFrame.from_dict(Counter(train_tweets.polarity.values),\n",
    "                                             orient='index')\n",
    "                                  .rename(columns={0: 'num_examples'}))\n",
    "class_distribution.index.name = 'class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distribution['perc_examples'] = np.around(class_distribution.num_examples /\n",
    "                                                np.sum(class_distribution.num_examples),\n",
    "                                                2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Exploration du texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ATTENTION :** Ne pas regarder les données de test !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets['text'].values[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (B) Représentation des textes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripteurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Du texte au numérique:**\n",
    "\n",
    "- les descripteurs (ou encore variables/features/traits/...) sont des unités textuelles : lemmes, racines ou autres ; prises individuellement ou en séquences (n-grammes), mais sans égard à leur ordre ou relations : c'est l'approche en **\"sac de mots\"** ;\n",
    "\n",
    "- les valeurs de ces variables sont numériques : binaires (présence/absence du descripteur dans le texte), numériques discrètes (nombre d'occurrences du descripteur dans le texte), numériques continues (différentes pondérations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texte vs corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Traditionnellement : les descripteurs textuels sont calculés sur l'ensemble du corpus. Tous les textes sont représentés par le mêmes ensemble de descripteurs, ce qui fait que la représentation d'un **texte** est un grand **vecteur épars** de taille fixe (taille du vecteur = taille du vocabulaire du corpus).\n",
    "\n",
    "+ Dans notre cas, un texte est un tweet et le corpus est l'ensemble des tweets. Le vocabulaire du corpus est l'ensemble des types (i.e. tokens pris une seule fois) apparaissant dans au moins un tweet.\n",
    " \n",
    "+ **Attention :** le vocabulaire du corpus d'entraînement est aussi celui utilisé pour la prédiction !\n",
    "\n",
    "Aujourd'hui (état de l'art) : chaque **mot** est représenté par un **vecteur dense** de valeurs réelles. Le texte est représenté par une aggrégation sous une certaine forme des vecteurs de ses mots constituants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sélection de descripteurs : prétraitements textuels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectif : réduire le nombre de descripteurs : réduire à un seul descripteur ceux qui sont équivalents (p. ex. deux mots qui ont été écrits avec et sans accents respectivement) ou qui peuvent être regroupés dans une classe d'équivalence (p. ex. remplacer toutes les instances de date par un mot fictif, p. ex. \"DATEEXPR\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques procédés : lemmatisation, racinisation, normalisation/correction orthographique, suppression des accents, mise en minuscules, suppression de la ponctuation, suppression de certains mots (mots dits \"vides\", autres mots), substitution de certains mots par un autre représentant leur appartenance à une classe, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la pratique certains de ces procédés sont souvent appliqués ensemble ou bien ils peuvent être pris en charge par la boîte à outils dans le workflow d'apprentissage automatique (divers paramètres à spécifier à certaines étapes du processus), qui les applique alors en boîte noire. Cependant tous les outils ne gèrent pas au même niveau l'anglais et les autres langues (dont le français), pour lesquelles toutes les options proposées pour l'anglais ne sont pas toujours disponibles.\n",
    "\n",
    "D'autre part, le choix d'appliquer ou non un certain procédé doit prendre en compte les besoins du contexte concret (p. ex. une mise en minuscules affecte-t-elle la reconnaissance d'entités nommées, si celle-ci est préconisée ?). Pour notre tâche : comment gérer les hashtags, les URL, les noms d'utilisateur (ou pseudos), etc. ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme entraînement à la manipulation du texte, nous verrons quelques exemples de transformation du texte. Nous produirons plusieurs versions de nos textes, qui pourront servir par la suite à l'étape d'apprentissage du classifieur et de prédiction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour illustrer l'effet des différentes transformations sur le texte, prenons comme exemple ce tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple\n",
    "tw = train_tweets['text'].iloc[100]\n",
    "tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_nlp = nlp(tw) # spacy\n",
    "tw_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(tw_nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Pas de sélection : mots tels quels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in tw_nlp:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Réduction par regroupement/uniformisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1. Lemmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in tw_nlp:\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCICE.** Produire une version lemmatisée des tweets et la mettre dans une colonne `lemmas` dans la dataframe. Pour ce faire : créer une fonction qui lemmatise un texte ; appliquer cette fonction à la colonne `text` des deux dataframes (`train` et `test`). **Attention**, le traitement de toute la colonne peut prendre un peu de temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatise_text(text):\n",
    "    text = nlp(text)\n",
    "    lemmas = [token.lemma_ for token in text]\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatise_text(tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets['lemmas'] = train_tweets['text'].apply(lemmatise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets['lemmas'] = test_tweets['text'].apply(lemmatise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde\n",
    "train_tweets.to_pickle('train.pkl')\n",
    "test_tweets.to_pickle('test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2. Racines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk.stem import SnowballStemmer # ou: from nltk.stem.snowball import FrenchStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('french')\n",
    "tokenizer = TweetTokenizer()\n",
    "tokenizer.tokenize(tw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les tweets présentent des particularités par rapport à d'autres textes. Des outils conçus spécifiquement pour gérer ce type de texte existent. Par exemple, `nltk` propose un tokeniseur pour les tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True) \n",
    "# strip_handles supprime les @...\n",
    "# reduce_len réduit les séquences de caractères répétés plus de trois fois à des séquences de taille trois\n",
    "tokenizer.tokenize(tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in tokenizer.tokenize(tw):\n",
    "    print(stemmer.stem(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCICE.** Produire une version racinisée des tweets et la mettre dans une colonne `stems` dans la dataframe. Pour ce faire : créer une fonction qui racinise un texte ; appliquer cette fonction à la colonne `text` des deux dataframes (`train` et `test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text):\n",
    "    tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    stemmer = SnowballStemmer('french')\n",
    "    stems = [stemmer.stem(token) for token in tokenizer.tokenize(text)]\n",
    "    return ' '.join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_text(tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets['stems'] = train_tweets['text'].apply(stem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets['stems'] = test_tweets['text'].apply(stem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde\n",
    "train_tweets.to_pickle('train.pkl')\n",
    "test_tweets.to_pickle('test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3. Étiquettes morphosyntaxiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in tw_nlp:\n",
    "    print(f'{token.text}\\t{token.pos_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCICE.** Produire une version des tweets où chaque token est remplacé par son étiquette morphosyntaxique et la mettre dans une colonne `pos` dans la dataframe. Pour ce faire : créer une fonction qui remplace les mots par leurs étiquettes dans un texte ; appliquer cette fonction à la colonne `text` des deux dataframes (`train` et `test`). Attention, le traitement de toute la colonne peut prendre un peu de temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_words_with_pos_tag(text):\n",
    "    text = nlp(text)\n",
    "    return ' '.join([token.pos_ for token in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_words_with_pos_tag(tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets['pos'] = train_tweets['text'].apply(replace_words_with_pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets['pos'] = test_tweets['text'].apply(replace_words_with_pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde\n",
    "train_tweets.to_pickle('train.pkl')\n",
    "test_tweets.to_pickle('test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4. Classe d'appartenance des entités nommées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = pd.read_pickle('train.pkl')\n",
    "test_tweets = pd.read_pickle('test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limites : reconnaissance imparfaite. Faire des essais pour appréhender les limites de l'outil. Cela nous permettra, par exemple, de corriger certaines erreurs systématiques de l'outil en intervenant en amont sur le texte pour transformer les éléments qui posent difficulté. Par exemple, dans notre cas, l'outil semble ne pas bien gérer les URL. On peut donc penser à les normaliser avant d'appliquer la reconnaissance d'entités nommées. Par ailleurs, un nettoyage du texte en amont peut aussi aider à une meilleure segmentation en tokens et, par conséquent, possiblement aussi à une meilleure reconnaissance des entités nommées (REN, ou NER en anglais)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_nlp.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_nlp.ents[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCICE.** Produire une version des tweets où chaque entité nommée est remplacée par sa classe et la mettre dans une colonne `entites_nommees` dans la dataframe. Pour ce faire : créer une fonction qui remplace les entités nommées reconnues par leurs classe dans un texte ; appliquer cette fonction à la colonne `text` des deux dataframes (`train` et `test`). Attention, le traitement de toute la colonne peut prendre un peu de temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(text):\n",
    "    original_text = nlp(text)\n",
    "    entities = {ent.text: ent.label_ for ent in original_text.ents}\n",
    "    new_text = ''\n",
    "    for token in original_text:\n",
    "        token = token.text\n",
    "        if token in entities:\n",
    "            label = entities[token]\n",
    "            new_text += label + ' '\n",
    "        else:\n",
    "            new_text += token + ' '\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner(tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets['entites_nommees'] = train_tweets['text'].apply(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets['entites_nommees'] = test_tweets['text'].apply(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde\n",
    "train_tweets.to_pickle('train.pkl')\n",
    "test_tweets.to_pickle('test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.5. Autres classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme exemple, nous remplacerons les adresses Web par un mot fictif URLEXPR. N'hésitez pas à penser à d'autres classes d'équivalence qui vous semblent pertinentes (dates, prix, etc.) !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCICE.** Créer une fonction `substitute_url` qui prend en entrée une chaîne de caractères et le mot de remplacement (p. ex. \"URLEXPR\") et remplace les URL présentes dans la chaîne de caractères par le mot de remplacement donné en argument. Indication : utiliser des expressions régulières (module `re`) ; examiner des exemples de tweets (du corpus \n",
    "d'entraînement) pour bien saisir la structure des URL. Appliquer ensuite cette fonction à la colonne `text` des dataframes d'entraînement et de test. Dans les deux cas, stocker le résultat de la transformation dans une nouvelle colonne `sans_url`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitute_url(text, url_replacement):\n",
    "    text = re.sub(r'https?:\\S+', url_replacement, text) # http://t.co/eFKkE9W0GI\n",
    "    text = re.sub(r'\\bwww\\.\\S+', url_replacement, text) # www.example.com\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets['sans_url'] = train_tweets['text'].apply(substitute_url,\n",
    "                                                      url_replacement='URLEXPR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets['sans_url'] = test_tweets['text'].apply(substitute_url,\n",
    "                                                    url_replacement='URLEXPR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets.shape, test_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde\n",
    "train_tweets.to_pickle('train.pkl')\n",
    "test_tweets.to_pickle('test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Réduction par filtrage : suppression de certains mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1. Filtrage des mots par fréquence d'utilisation en langue générale : \"mots vides\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Critère vague, la notion de mot vide pouvant varier selon le contexte : mots très fréquents en langue générale ou dans un corpus particulier. Ce filtrage est en général géré lors de la vectorisation du corpus (voir plus bas, pour le calcul des valeurs des descripteurs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = nltk.corpus.stopwords.words('french')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques améliorations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'les' in sw # omission importante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw.append('les')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'les' in sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2. Filtrage des mots par contenu expressif : mots qui n'ont pas une polarité attestée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pourrez, comme exercice optionnel, implémenter une méthode basée sur ce type de filtrage (voir avant-dernière section de ce notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calcul des valeurs des descripteurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de procéder aux calculs, nous séparerons un jeu de données de validation à partir des données d'entraînement initiales. Nous mettons les données de test fournies de côté, exclusivement pour une évaluation finale des modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train_tweets['text'],train_tweets['polarity'],train_size=0.75, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = test_tweets['text'], test_tweets['polarity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Binaire : présence/absence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Étudier la documentation du constructeur de la classe. Il propose notamment une pondération binaire (les valeurs sont alors 0 ou 1) ou par valeurs entières (décomptes d'occurrence - voir section suivante).\n",
    "\n",
    "En plus, de nombreuses options de réduction du vocabulaire sont proposées. En voici quelques-unes :\n",
    "\n",
    "- suppression des accents : `strip_accents` ;\n",
    "\n",
    "- mise en minuscule : `lowercase` (par défault `True`) ;\n",
    "\n",
    "- seuillage sur la fréquence documentaire (c.à.d. le nombre de documents dans lesquels le terme apparaît) ; exemple : `max_df=0.7` signifie qu'on ignore les termes qui sont présentes dans plus de 70% des textes du corpus (ce qui équivaut à l'élimination des mots vides propres au corpus) ; `min_df=5` ignore les termes qui apparaissent dans moins de 5 textes du corpus ;\n",
    "\n",
    "- seuillage du nombre de variables à retenir : `max_features=1000` ne retient que les 1000 termes qui ont les \"term frequency\" (nombre d'occurrences dans un texte particulier) les plus élevées ;\n",
    "\n",
    "- suppression de mots vides : `stop_words` (liste par défaut ou fournie) ;\n",
    "\n",
    "- ordre des n-grammes : `ngram_range=(min_n, max_n)` extrait les n-grammes dont la taille est entre `min_n` et `max_n` (les deux compris)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_count = CountVectorizer(binary=True).fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized_bin = bin_count.transform(X_train)\n",
    "X_train_vectorized_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le corpus de validation et celui de test doivent être également transformés en matrices document-termes. **Très important :** les termes sont ceux décomptés sur le corpus d'entraînement ; les termes présent dans le corpus de validation ou de test mais absents du corpus d'entraînement seront ignorés. **Ne pas réapprendre donc le vectoriseur !!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_vectorized_bin = bin_count.transform(X_valid)\n",
    "X_test_vectorized_bin = bin_count.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_vectorized_bin # même nombre de \"colonnes\" (mots) que X_train_vectorized_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Numérique discret : décomptes d'occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer # déjà importé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul des fréquences d'occurrence (tf) des termes dans le corpus, avec les options par défaut :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_count = CountVectorizer().fit(X_train) # binary=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examinons le vocabulaire de notre corpus :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_count.get_feature_names()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_count.get_feature_names()[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vect_count.get_feature_names()) # taille du vocabulaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création de la matrice document-termes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized_count = vect_count.transform(X_train)\n",
    "X_train_vectorized_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme plus haut : transformation des corpus de validation et de test en matrices document-termes, **avec le même vectoriseur**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_vectorized_count = vect_count.transform(X_valid)\n",
    "X_test_vectorized_count = vect_count.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fois-ci nous allons inclure des bigrammes dans le vocabulaire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_count_bigrams = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n",
    "X_train_vectorized_count_bigrams = vect_count_bigrams.transform(X_train)\n",
    "X_valid_vectorized_count_bigrams = vect_count_bigrams.transform(X_valid)\n",
    "X_test_vectorized_count_bigrams = vect_count_bigrams.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vect_count_bigrams.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Numérique continu : TF-IDF (ou autres pondérations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitons le vocabulaire à des termes qui apparaissent dans au moins 5 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_tfidf = TfidfVectorizer(min_df=5).fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La réduction de la taille du vocabulaire est spectaculaire !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vect_count.get_feature_names()), len(vect_tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized_tfidf = vect_tfidf.transform(X_train)\n",
    "X_valid_vectorized_tfidf = vect_tfidf.transform(X_valid)\n",
    "X_test_vectorized_tfidf = vect_tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Classification des textes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous entraînerons des modèles de classification appartenant à quelques familles d'algorithmes d'apprentissage automatique. D'autres familles restent à explorer. L'objectif est de comparer non seulement les performances des différentes méthodes entre elles, mais aussi la performance d'une même méthode sur des représentations différentes du texte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Schéma général :** apprentissage (> évaluation sur données de validation > apprentissage > évaluation sur données de validation >...) (> évaluation sur données de test) > prédiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boîte à outils (algorithmes pré-implémentés) : `scikit-learn`. Interface unifiée pour l'ensemble des algorithmes. **Mise en oeuvre :**\n",
    "\n",
    "* si besoin de réduire et/ou centrer les données (réduction statistique) :\n",
    "    - création d'un objet `scaler` de la classe adaptée (réducteur des données) ;\n",
    "    - entraînement du `scaler` sur les données d'entraînement : méthode `fit` de l'objet réducteur ;\n",
    "    - réduction des données d'entraînement : méthode `transform` du réducteur ; cette étape peut être enchaînée avec la précédente grâce à la méthode `fit_transform` du réducteur ;\n",
    "    - réduction des données de validation et de test : méthode `transform` du réducteur (attention : même réducteur que pour les données d'entraînement ! On ne réapprend pas les critères de réduction sur les données de validation/test !) ;\n",
    "\n",
    "\n",
    "* création de l'objet estimateur : appel du constructeur de la classe pertinente, avec d'éventuels paramètres si valeurs autres que défaut ;\n",
    "\n",
    "* apprentissage de l'estimateur sur les données d'entraînement (éventuellement réduites) : méthode `fit` de l'estimateur ; cette étape peut être enchaînée avec la précédente ;\n",
    "\n",
    "* évaluation de l'estimateur sur les données d'entraînement, de validation et/ou (uniquement si c'est le modèle final !) de test : méthode `score` de l'estimateur.\n",
    "\n",
    "* prédiction sur des données nouvelles : méthode `predict` de l'estimateur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Métriques d'évaluation :**\n",
    "\n",
    "* taux de bonne classification (*accuracy*) ;\n",
    "\n",
    "* précision (*precision*) ;\n",
    "\n",
    "* rappel (*recall*) ;\n",
    "\n",
    "* score F1 ;\n",
    "\n",
    "* aire sous la courbe ROC (*ROC AUC*) : pour la classification binaire ;\n",
    "\n",
    "* métriques \"maison\", sur mesure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Modèles de référence faibles (*weak baselines*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Choix aléatoire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toutes les classes ont les mêmes chances d'être choisies (prédiction uniforme) ou bien le prédicteur respecte la distribution des classes dans les données d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prédiction proportionnelle à la distribution des classes dans les données d'entraînement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_prop_class = DummyClassifier(strategy='stratified').fit(X_train_vectorized_tfidf,\n",
    "                                                               y_train)\n",
    "predictions_valid = random_prop_class.predict(X_valid_vectorized_tfidf)\n",
    "conf_mat = confusion_matrix(y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prédiction uniforme :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_uniform = DummyClassifier(strategy='uniform').fit(X_train_vectorized_tfidf,\n",
    "                                                         y_train)\n",
    "predictions_valid = random_uniform.predict(X_valid_vectorized_tfidf)\n",
    "predictions_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_valid, predictions_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Prédiction constante de la classe majoritaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seules les instances de la classe majoritaire seront classées correctement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCICE.** Quel est le taux de bonne classification pour cette approche dans notre scénario ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifiez votre réponse :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maj = DummyClassifier(strategy='most_frequent').fit(X_train_vectorized_tfidf, y_train)\n",
    "predictions_valid = maj.predict(X_valid_vectorized_tfidf)\n",
    "predictions_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maj_class = (class_distribution.index[class_distribution.perc_examples ==\n",
    "                                      np.amax(class_distribution.perc_examples)][0])\n",
    "maj_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(predictions_valid == maj_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maj.score(X_valid_vectorized_tfidf, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_valid, predictions_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2. Classifieur naïf bayesien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En général pris également comme baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nb = MultinomialNB().fit(X_train_vectorized_tfidf, y_train)\n",
    "predictions_valid = model_nb.predict(X_valid_vectorized_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_valid, predictions_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCICE.** Entraîner un modèle avec les arguments suivants : `multi_class='multinomial'`, `solver='lbfgs'` sur le corpus vectorisé par nombre d'occurrences et l'évaluer sur le corpus de validation. Si vous recevez un message indiquant que l'algorithme d'optimisation a du mal à converger, augmentez le nombre d'itérations (paramètre `max_iter`, par défaut 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression(multi_class='multinomial', solver='lbfgs',\n",
    "                              max_iter=200).fit(X_train_vectorized_count, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_valid = model_lr.predict(X_valid_vectorized_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_valid, predictions_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examinons les variables (termes) ayant l'association la plus forte avec chaque classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_n_strongly_associated_features(vectoriser, model, n):\n",
    "    feature_names = np.array(vectoriser.get_feature_names())\n",
    "\n",
    "    for i in range(3):\n",
    "        class_name = model.classes_[i]\n",
    "        print(\"CLASSE {}\".format(class_name))\n",
    "        idx_coefs_sorted = model.coef_[i].argsort() # ordre croissant\n",
    "        print(\"Les dix variables ayant l'association négative la plus forte \" + \n",
    "              \"avec la classe {} :\\n{}\\n\".format(class_name,\n",
    "                                                 feature_names[idx_coefs_sorted[:n]]))\n",
    "        idx_coefs_sorted = idx_coefs_sorted[::-1] # ordre décroissant\n",
    "        print(\"Les dix variables ayant l'association positive la plus forte \" +\n",
    "              \"avec la classe {} :\\n{}\\n\"\n",
    "              .format(class_name,\n",
    "                      feature_names[idx_coefs_sorted[:n]]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_n_strongly_associated_features(vect_count, model_lr, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCICE.** Entraîner un modèle avec les arguments suivants : `multi_class='multinomial'`, `solver='lbfgs'` sur le corpus vectorisé par TF-IDF et l'évaluer sur le corpus de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression(multi_class='multinomial',\n",
    "                              solver='lbfgs').fit(X_train_vectorized_tfidf, y_train)\n",
    "predictions_valid = model_lr.predict(X_valid_vectorized_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_valid, predictions_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La performance est légèrement inférieure, mais nous l'avons obtenue en utilisant considérablement moins de variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = np.array(vect_tfidf.get_feature_names())\n",
    "idx_tfidf_sorted = X_train_vectorized_tfidf.max(0).toarray()[0].argsort()\n",
    "print(\"TF-IDF le moins élevé : {}\".format(feature_names[idx_tfidf_sorted[:10]]))\n",
    "print(\"TF-IDF le plus élevé : {}\".format(feature_names[idx_tfidf_sorted[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec le vectoriseur à **unigrammes et bigrammes** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression(multi_class='multinomial',\n",
    "                              solver='lbfgs').fit(X_train_vectorized_count_bigrams,\n",
    "                                                  y_train)\n",
    "predictions_valid = model_lr.predict(X_valid_vectorized_count_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_valid, predictions_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_n_strongly_associated_features(vect_count_bigrams, model_lr, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svm = SVC(kernel='linear', C=0.1).fit(X_train_vectorized_count_bigrams, y_train)\n",
    "predictions_valid = model_svm.predict(X_valid_vectorized_count_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_valid, predictions_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercices optionnels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implémentation d'une méthode sans apprentissage automatique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette méthode, qui pourrait servir de modèle de référence forte (*strong baseline*) pour notre tâche, utilise  un lexique qui recense des mots et leur polarité. Elle consiste à calculer la polarité globale d'un texte en agrégeant les polarités des mots qu'il contient. Concrètement, dans sa version la plus simplifiée, cette méthode calcule la polarité d'un texte selon la formule : sign(nombre_de_termes_positifs - nombre_de_termes_négatifs), donc -1 si la somme est négative, 0 si elle est nulle et 1 si elle est positive ([Hu et Liu (2004)](https://pdfs.semanticscholar.org/13e5/f0c40c85ca8e01b3756963d5352358de7c29.pdf), [Kim et Hovy (2004)](http://anthology.aclweb.org/P/P06/P06-2.pdf#page=493)). Cela revient à un filtrage des mots, suivi d'un calcul de score. Des variantes affinées de cette approche ont également été proposées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentez cette approche et testez-la sur nos corpus, en vous servant du lexique [FEEL](http://advanse.lirmm.fr/feel.php) (voir [article](https://hal-lirmm.ccsd.cnrs.fr/lirmm-01348016/document)) : une liste de 14128 **lemmes** annotés en termes de polarité (positif/négatif) et de six émotions (joy, fear, sadness, anger, surprise, disgust). Pour cet exercice seule la colonne polarité du lexique est à retenir. Faites bien attention à ce que les formes lexicales présentes dans le texte et celles du lexique soient comparables !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez aussi, éventuellement, explorer l'attribut `sentiment` des tokens générés par `spacy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Réflection : cette approche vous semble-t-elle raisonnable ? Quelle est sa performance si on la compare à celle des méthodes basées sur l'apprentissage automatique que vous avez testées ? Quels en sont, d'après vous, les points forts et les points faibles ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sélection de modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les algorithmes que vous avez testés (ou d'autres, voir les pistes pour approfondissement), essayez de trouver une combinaison des hyper-paramètres et de la représentation des textes qui entraîne une amélioration de la performance. Pouvez-vous atteindre un taux de bonne classification (*accuracy*) d'au moins 70% sur le corpus de validation ?\n",
    "\n",
    "Une fois votre modèle choisi sur la base de sa performance sur le corpus de validation, évaluez-le sur le corpus de test et rapportez le résultat. Cette évaluation est à faire une seule fois ! Elle est censée donner une estimation aussi fiable que possible du pouvoir de généralisation de votre modèle. Il n'est pas question d'affiner encore le modèle après cette évaluation, puis de l'évaluer à nouveau sur les données de test : à ce moment-là, votre corpus de test sera devenu un simple corpus de validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pistes pour l'approfondissement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Pousser la réduction de variables plus loin : correction/normalisation de l'orthographe, autres transformations considérées pertinentes. Il n'y a pas de recette universelle, il faut essayer différentes approches et voir ce qui marche le mieux sur nos textes et notre tâche. Le texte des tweets pose des problèmes particuliers, il faut donc trouver les traitements les mieux adaptés.\n",
    "\n",
    "**2.** Combiner des descripteurs textuels avec des variables non-textuelles. Il faudra construire explicitement (pas en version sparse matrix) la matrice document-termes (et appliquer probablement un filtrage plus agressif, pour des raisons de coût de mémoire) pour pouvoir l'augmenter d'autres variables. Le fichier `json` correspondant à chaque tweet contient des méta-données. En choisir une ou deux, les extraire et les ajouter à la représentation des données. Puis entraîner et évaluer un classifieur sur ce nouveau jeu de descripteurs.\n",
    "\n",
    "**3.** Essayer d'autres algorithmes de classification, par exemple : arbres de décision (`from sklearn.tree import DecisionTreeClassifier`), méthodes d'ensemble (forêts aléatoires : `from sklearn.ensemble import RandomForestClassifier` ; gradient-boosted decision trees : `from sklearn.ensemble import GradientBoostingClassifier`), réseaux de neurones simple (`from sklearn.neural_network import MLPClassifier`) ou bien des architectures plus complexes (autres librairies : `tensorflow`, `keras`, etc.).\n",
    "\n",
    "**4.** Ajuster les hyper-paramètres d'un modèle par validation croisée (`from sklearn.model_selection import GridSearchCV`). Utiliser dans ce cas la totalité du jeu d'entraînement fourni initialement (ne plus en séparer une portion pour la validation)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
